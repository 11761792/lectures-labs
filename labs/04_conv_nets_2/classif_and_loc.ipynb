{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell before the lab !\n",
    "# It will download PascalVOC dataset (400Mo) and \n",
    "# pre-computed representations of images (450Mo)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "\n",
    "import tarfile\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2 compat\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "URL_VOC = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n",
    "FILE_VOC = \"VOCtrainval_06-Nov-2007.tar\"\n",
    "FOLDER_VOC = \"VOCdevkit\"\n",
    "\n",
    "if not op.exists(FILE_VOC):\n",
    "    print('Downloading from %s to %s...' % (URL_VOC, FILE_VOC))\n",
    "    urlretrieve(URL_VOC, './' + FILE_VOC)\n",
    "\n",
    "if not op.exists(FOLDER_VOC):\n",
    "    print('Extracting %s...' % FILE_VOC)\n",
    "    tar = tarfile.open(FILE_VOC)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "URL_REPRESENTATIONS = \"https://github.com/m2dsupsdlclass/lectures-labs/releases/download/0.1/voc_representaions.h5\"\n",
    "FILE_REPRESENTATIONS = \"voc_representaions.h5\"\n",
    "\n",
    "if not op.exists(FILE_REPRESENTATIONS):\n",
    "    print('Downloading from %s to %s...' % (URL_REPRESENTATIONS, FILE_REPRESENTATIONS))\n",
    "    urlretrieve(URL_REPRESENTATIONS, './' + FILE_REPRESENTATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Localisation model\n",
    "\n",
    "The objective is to build and train a classification and localisation network. This exercise will showcase the flexibility of Deep Learning with several, heterogenous outputs (bounding boxes and classes)\n",
    "\n",
    "The model is in two parts:\n",
    "- Representations from pre-trained ResNet50 network `shape = (7, 7, 2048)`\n",
    "- A simplified model which outputs \n",
    "  - classes (5 classes)\n",
    "  - bounding box coordinates\n",
    "\n",
    "## Loading images and annotations\n",
    "\n",
    "We will be using pascalVOC 2007, a dataset widely used in detection and segmentation http://host.robots.ox.ac.uk/pascal/VOC/ To lower memory footprint and training time, we'll only use 5 classes: \"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\". Here are the first steps:\n",
    "- Load the annotations file from pascalVOC and parse it (xml file)\n",
    "- keep only the annotations we're interested in, and containing a single object\n",
    "- Pre-compute ResNet conv5c from the corresponding images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lxml import etree\n",
    "import os\n",
    "\n",
    "# Parse the xml annotation file and retrieve the path to image, its size and annotations\n",
    "def extract_xml_annotation(filename):\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall(\"/object\")\n",
    "    size = (int(z.find(\"//width\").text), int(z.find(\"//height\").text))\n",
    "    fname = z.find(\"/filename\").text\n",
    "    dics = [{obj.find(\"name\").text:[int(obj.find(\"bndbox/xmin\").text), \n",
    "                                    int(obj.find(\"bndbox/ymin\").text), \n",
    "                                    int(obj.find(\"bndbox/xmax\").text), \n",
    "                                    int(obj.find(\"bndbox/ymax\").text)]} \n",
    "            for obj in objects]\n",
    "    output = {\"size\": size, \"filename\":fname, \"objects\":dics}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filters annotations keeping only those we are interested in\n",
    "# We only keep images in which there is a single item\n",
    "annotations = []\n",
    "\n",
    "filters = [\"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\"]\n",
    "idx2labels = {k:v for (k,v) in enumerate(filters)}\n",
    "labels2idx = {v:k for k,v in idx2labels.items()}\n",
    "\n",
    "for file in os.listdir(\"VOCdevkit/VOC2007/Annotations/\"):\n",
    "    annotation = extract_xml_annotation(\"VOCdevkit/VOC2007/Annotations/\" +file)\n",
    "    new_objects = []\n",
    "    for obj in annotation[\"objects\"]:\n",
    "        # keep only labels we're interested in\n",
    "        if list(obj.keys())[0] in filters:\n",
    "            new_objects.append(obj)\n",
    "    # Keep only if there's a single object in the image\n",
    "    if len(new_objects)==1:\n",
    "        annotation[\"class\"] = list(new_objects[0].keys())[0]\n",
    "        annotation[\"bbox\"] = list(new_objects[0].values())[0]\n",
    "        annotation.pop(\"objects\")\n",
    "        annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"number of images/annotations:\", len(annotations))\n",
    "print(\"\\nexample annotation[0]:\\n\", annotations[0])\n",
    "print(\"\\ncorrespondance between indexes and labels:\", idx2labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing representations\n",
    "\n",
    "Load a headless pre-trained ResNet50:\n",
    "- loading a headless ResNet from Keras and removing the AveragePooling layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/load_pretrained.py\n",
    "model = ResNet50(include_top=False)\n",
    "input = model.layers[0].input\n",
    "\n",
    "# Remove the average pooling layer\n",
    "output = model.layers[-2].output\n",
    "headless_conv = Model(input = input, output = output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict on a batch of images\n",
    "\n",
    "The `predict_batch` function is defined as follows:\n",
    "- open each image, and resize them to `img_size`\n",
    "- stack them as a batch tensor of shape `(batch, img_size_x, img_size_y, 3)`\n",
    "- preprocess the batch and make a forward pass with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def predict_batch(model, img_batch_path, img_size=None):\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in img_batch_path:\n",
    "        img = imread(im_path)\n",
    "        if img_size:\n",
    "            img = imresize(img,img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        img_list.append(img)\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    batch = preprocess_input(img_batch)\n",
    "    return model.predict(x = img_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test our model\n",
    "output = predict_batch(headless_conv, [\"dog.jpg\"], (1000, 224))\n",
    "print(\"output shape\", output.shape)\n",
    "\n",
    "# The output size is (batch_size, 1000/32 = 32, 224/32 = 7, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute representations on all images in our annotations\n",
    "\n",
    "Computing representations for all images may take some time, so it was pre-computed and save in `voc_representaions.h5`\n",
    "\n",
    "This was achieved through the `compute_representations.py` script, you're welcome to use it if needed.\n",
    "\n",
    "Otherwise, load the pre-trained representations in h5 format using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load pre-calculated representations\n",
    "h5f = h5py.File('voc_representaions.h5','r')\n",
    "reprs = h5f['reprs'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ground truth from annotation\n",
    "\n",
    "We cannot use directly the annotation dictionnary as ground truth in our model. \n",
    "\n",
    "We will build the `y_true` tensor that will be compared to the output of the model\n",
    "\n",
    "#### boxes\n",
    "- The image is resized to a fixed 224x224, so need to be the boxes coordinates\n",
    "- We have to convert the top-left and bottom-right coordinates `(x1, y1, x2, y2)` to center, height, width `(xc,yc,w,h)`\n",
    "\n",
    "#### classes\n",
    "- the class labels are mapped to corresponding indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_resize = 224\n",
    "num_classes = len(labels2idx.keys())\n",
    "\n",
    "def box_center(x,y):\n",
    "    return (box_size/2 + x * box_size, box_size/2 + y * box_size)\n",
    "\n",
    "def convert_to_ground_truth(annotations):\n",
    "    all_boxes = []\n",
    "    all_cls = []\n",
    "    for idx, annotation in enumerate(annotations):\n",
    "        # Build a one-hot encoding of the class\n",
    "        cls = np.zeros((num_classes))\n",
    "        cls_idx = labels2idx[annotation[\"class\"]]\n",
    "        cls[cls_idx] = 1.0\n",
    "        \n",
    "        coords = annotation[\"bbox\"]\n",
    "        size = annotation[\"size\"]\n",
    "        # resize the image\n",
    "        x1, y1, x2, y2 = (coords[0]*img_resize/size[0], coords[1]*img_resize/size[1], \n",
    "                          coords[2]*img_resize/size[0], coords[3]*img_resize/size[1])\n",
    "        \n",
    "        #compute center height and width\n",
    "        cx, cy = ((x2 + x1)/2, (y2 + y1)/2)\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        boxes = np.array([cx, cy, w, h])\n",
    "        all_boxes.append(boxes)\n",
    "        all_cls.append(cls)\n",
    "    # stack everything into two big np tensors\n",
    "    return np.stack(all_cls, axis=0), np.stack(all_boxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes, boxes = convert_to_ground_truth(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"classes and boxes shapes:\", classes.shape, boxes.shape)\n",
    "print(\"\\n classes examples:\\n\" , classes[0:2])\n",
    "print(\"\\n boxes examples:\\n\" , boxes[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting output of model\n",
    "\n",
    "Interpreting the output of the model is going from the output tensors to a set of classes (with confidence) and boxes coordinates. It corresponds to reverting the previous process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpret_output(cls, boxes, img_size=(500,333)):\n",
    "    cls_idx = np.argmax(cls)\n",
    "    confidence = cls[cls_idx]\n",
    "    classname = idx2labels[cls_idx]\n",
    "    cx, cy = boxes[0], boxes[1]\n",
    "    w, h = boxes[2], boxes[3]\n",
    "    \n",
    "    small_box = [max(0, cx - w/2), max(0, cy - h/2), \n",
    "                 min(img_resize, cx + w/2), min(img_resize, cy + h/2)]\n",
    "    \n",
    "    fullsize_box = [int(small_box[0] * img_size[0] / img_resize), \n",
    "                    int(small_box[1] * img_size[1] / img_resize),\n",
    "                    int(small_box[2] * img_size[0] / img_resize), \n",
    "                    int(small_box[3] * img_size[1] / img_resize)]\n",
    "    output = {\"class\": classname, \"confidence\":confidence, \"bbox\": fullsize_box}\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sanity check: interpret the classes and boxes tensors\n",
    "\n",
    "print(annotations[1])\n",
    "output = interpret_output(classes[1], boxes[1], img_size=annotations[1][\"size\"])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "\n",
    "In order to assess the quality of our model, we will monitor the IoU between ground truth box and predicted box. \n",
    "The following function computes the IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    # find the intersecting box coordinates\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    " \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    " \n",
    "    # compute the area of each box\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of areas - the interesection area \n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"iou:\",iou(annotations[1][\"bbox\"], output[\"bbox\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Localisation model\n",
    "\n",
    "A two headed model for classification and localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def masked_mse(y_true, y_pred):\n",
    "    masks = K.not_equal(y_true, 0.)\n",
    "    return K.mean(K.square(y_pred - y_true) * K.cast(masks, \"float32\"), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.objectives import mean_squared_error, binary_crossentropy\n",
    "from keras.layers import Input, Convolution2D, Dropout, GlobalAveragePooling2D, Flatten, Dense, GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "def classif_and_loc_stupid(num_classes):\n",
    "    model_input = Input(shape=(7,7,2048))\n",
    "    \n",
    "    # rather stupid version: we average all spatial information\n",
    "    x = GlobalAveragePooling2D()(model_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    head_classes = Dense(num_classes, activation=\"softmax\", name=\"head_classes\")(x)\n",
    "    head_boxes = Dense(4, name=\"head_boxes\")(x)\n",
    "    \n",
    "    model = Model(model_input, output = [head_classes, head_boxes], name=\"resnet_loc\")\n",
    "    model.compile(optimizer=\"adam\", loss=[binary_crossentropy, \"mse\"], \n",
    "                  loss_weights=[1., 0.01]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = classif_and_loc_stupid(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debugging the model: select a few examples \n",
    "# and test the model (before training)\n",
    "# Check that shapes correspond to ground truth!\n",
    "\n",
    "num = 64\n",
    "inputs = reprs[0:num]\n",
    "out_cls, out_boxes = classes[0:num], boxes[0:num]\n",
    "print(\"input batch shape:\", inputs.shape, \"\\nground truth batch shapes:\", out_cls.shape, out_boxes.shape)\n",
    "out = model.predict(x=inputs)\n",
    "print(\"\\nmodel output shapes:\", out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debugging the model: check whether the loss decreases, \n",
    "# and eventually if we are able to overfit on these few examples\n",
    "history = model.fit(x = inputs, y=[out_cls, out_boxes], batch_size=10, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(history.history[\"head_boxes_loss\"]))\n",
    "plt.plot(np.log(history.history[\"head_classes_loss\"]))\n",
    "plt.plot(np.log(history.history[\"loss\"]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying images and bounding box\n",
    "\n",
    "In order to display our annotations, we build the function `plot_annotations` as follows:\n",
    "- display the image\n",
    "- display on top annotations and ground truth bounding boxes and classes\n",
    "\n",
    "The `display` function:\n",
    "- takes a single index and computes the result of the model\n",
    "- interpret the output of the model as a bounding box\n",
    "- calls the `plot_annotations` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def patch(axis, bbox, display_txt, color):\n",
    "    coords = (bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1\n",
    "    axis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    axis.text(bbox[0], bbox[1], display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "    \n",
    "def plot_annotations(img_path, annotation=None, ground_truth=None):\n",
    "    img = imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    currentAxis = plt.gca()\n",
    "    if ground_truth:\n",
    "        text = \"gt \" + ground_truth[\"class\"]\n",
    "        patch(currentAxis, ground_truth[\"bbox\"], text, \"red\")\n",
    "    if annotation:\n",
    "        conf = '{:0.2f} '.format(annotation['confidence'])\n",
    "        text = conf + annotation[\"class\"]\n",
    "        patch(currentAxis, annotation[\"bbox\"], text, \"blue\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display(index, ground_truth=True):\n",
    "    res = model.predict(reprs[index][np.newaxis,])\n",
    "    output = interpret_output(res[0][0], res[1][0], img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     output, annotations[index] if ground_truth else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display one of the image on which we trained the model\n",
    "display(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display an image we didn't train the model on\n",
    "display(194)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Accuracy\n",
    "\n",
    "for each example `(class_true, bbox_true)`, we consider it positive if and only if:\n",
    "- the argmax of `output_class` of the model is `class_true`\n",
    "- the IoU between the `output_bbox` and the `bbox_true` is above a threshold (usually `0.5`)\n",
    "\n",
    "The accuracy of a model is then number of positive / total_number\n",
    "\n",
    "The following functions compute the class accuracy, iou average and global accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute class accuracy, iou average and global accuracy\n",
    "def accuracy_and_iou(preds, trues, threshold = 0.5):\n",
    "    sum_valid, sum_accurate, sum_iou = 0, 0, 0\n",
    "    num = len(preds)\n",
    "    for pred, true in zip(preds, trues):\n",
    "        iou_value = iou(pred[\"bbox\"], true[\"bbox\"])\n",
    "        if pred[\"class\"]==true[\"class\"] and iou_value > threshold:\n",
    "            sum_valid = sum_valid + 1\n",
    "        sum_iou = sum_iou + iou_value\n",
    "        if pred[\"class\"]==true[\"class\"]:\n",
    "            sum_accurate = sum_accurate + 1\n",
    "    return sum_accurate/num, sum_iou/num, sum_valid/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the previous function on the whole train / test set\n",
    "def compute_acc(train=True):\n",
    "    if train:\n",
    "        beg, end = 0, (9 * len(annotations))\n",
    "        txt = \"train\"\n",
    "    else:\n",
    "        beg, end = (9 * len(annotations)) // 10, len(annotations) \n",
    "        txt = \"test\"\n",
    "    res = model.predict(reprs[beg:end])\n",
    "    outputs = []\n",
    "    for index, (classes, boxes) in enumerate(zip(res[0], res[1])):\n",
    "        output = interpret_output(classes, boxes, img_size=annotations[index][\"size\"])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    acc, iou, valid = accuracy_and_iou(outputs, annotations[beg:end], threshold=0.5)\n",
    "    \n",
    "    print(txt + ' acc: {:0.3f}, mean iou: {:0.3f}, acc_valid: {:0.3f}'.format(acc, iou, valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_acc(train=True)\n",
    "compute_acc(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the whole dataset\n",
    "\n",
    "We split our dataset into a train and a test dataset\n",
    "\n",
    "Then train the model on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Keep last examples for test\n",
    "test_num = reprs.shape[0] // 10\n",
    "train_num = reprs.shape[0] - test_num\n",
    "test_inputs = reprs[train_num:]\n",
    "test_cls, test_boxes = classes[train_num:], boxes[train_num:]\n",
    "print(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inputs = reprs[0:train_num]\n",
    "out_cls, out_boxes = classes[0:train_num], boxes[0:train_num]\n",
    "\n",
    "history = model.fit(x = inputs, y=[out_cls, out_boxes], \n",
    "                    validation_data=(test_inputs, [test_cls, test_boxes]), \n",
    "                    batch_size=batch_size, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_acc(train=True)\n",
    "compute_acc(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a better model\n",
    "\n",
    "**Exercise**\n",
    "Use any tool at your disposal to build a better model:\n",
    "- Dropout\n",
    "- Convolution2D, Dense, with activations functions\n",
    "- Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, etc.\n",
    "\n",
    "Notes:\n",
    "- Be careful not to add too parametrized layers as you only have ~1200 training samples\n",
    "- Feel free to modify hyperparameters: learning rate, optimizers, loss_weights\n",
    "\n",
    "**Bonus**\n",
    "- Add data augmentation: \n",
    "  - flip images\n",
    "  - add random crops before resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classif_and_loc(num_classes):\n",
    "    model_input = Input(shape=(7,7,2048))\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    model = Model(model_input, output = [head_classes, head_boxes], name=\"resnet_loc\")\n",
    "    model.compile(optimizer=\"adam\", loss=[binary_crossentropy, \"mse\"], \n",
    "                  loss_weights=[1., 1 / (224*224)]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build, train and compute accuracy\n",
    "model = classif_and_loc(5)\n",
    "\n",
    "history = model.fit(x = inputs, y=[out_cls, out_boxes], \n",
    "                    validation_data=(test_inputs, [test_cls, test_boxes]), \n",
    "                    batch_size=batch_size, nb_epoch=30)\n",
    "\n",
    "compute_acc(train=True)\n",
    "compute_acc(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/classif_and_loc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
