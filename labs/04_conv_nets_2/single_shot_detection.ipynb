{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-shot detection model\n",
    "\n",
    "The objective is to build and train a localisation network. This exercise will showcase the flexibility of Deep Learning with several, heterogenous outputs (bounding boxes and classes)\n",
    "\n",
    "The model is in two parts:\n",
    "- Representations from pre-trained ResNet50 network `shape = (7, 7, 2048)`\n",
    "- A simplified ssd (single shot detection) model which outputs \n",
    "  - classes (dogs / cats / background)\n",
    "  - bounding box coordinates\n",
    "\n",
    "## Loading images and annotations\n",
    "\n",
    "We will be using pascalVOC 2007, a dataset widely used in detection and segmentation http://host.robots.ox.ac.uk/pascal/VOC/ To lower memory footprint and training time, we'll only use 2 classes: cat and dog. Here are the first steps:\n",
    "- Download the pascalVOC in the present folder\n",
    "- Load the annotations file from pascalVOC and parse it (xml file), keeping only cats and dogs\n",
    "- Pre-compute resnet representations from the corresponding images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lxml import etree\n",
    "import os\n",
    "\n",
    "# Parse the xml annotation file and retrieve the path to image, its size and annotations\n",
    "def extract_xml_annotation(filename):\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall(\"/object\")\n",
    "    size = (int(z.find(\"//width\").text), int(z.find(\"//height\").text))\n",
    "    fname = z.find(\"/filename\").text\n",
    "    dics = [{obj.find(\"name\").text:[int(obj.find(\"bndbox/xmin\").text), \n",
    "                                    int(obj.find(\"bndbox/ymin\").text), \n",
    "                                    int(obj.find(\"bndbox/xmax\").text), \n",
    "                                    int(obj.find(\"bndbox/ymax\").text)]} \n",
    "            for obj in objects]\n",
    "    output = {\"size\": size, \"filename\": fname, \"objects\": dics}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filters annotations keeping only those we are interested in\n",
    "annotations = []\n",
    "\n",
    "filters = [\"dog\", \"cat\"]\n",
    "for file in os.listdir(\"VOCdevkit/VOC2007/Annotations/\"):\n",
    "    annotation = extract_xml_annotation(\"VOCdevkit/VOC2007/Annotations/\" +file)\n",
    "    new_objects = []\n",
    "    for obj in annotation[\"objects\"]:\n",
    "        if list(obj.keys())[0] in filters:\n",
    "            new_objects.append(obj)\n",
    "    if len(new_objects)>0:\n",
    "        annotation[\"objects\"] = new_objects\n",
    "        annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filters annotations keeping only those we are interested in\n",
    "annotations2 = []\n",
    "\n",
    "filters = [\"dog\", \"cat\", \"bus\", \"car\", \"aeroplane\"]\n",
    "for file in os.listdir(\"VOCdevkit/VOC2007/Annotations/\"):\n",
    "    annotation = extract_xml_annotation(\"VOCdevkit/VOC2007/Annotations/\" +file)\n",
    "    new_objects = []\n",
    "    for obj in annotation[\"objects\"]:\n",
    "        if list(obj.keys())[0] in filters:\n",
    "            new_objects.append(obj)\n",
    "    if len(new_objects)==1:\n",
    "        annotation[\"class\"] = list(new_objects[0].keys())[0]\n",
    "        annotation[\"bbox\"] = list(new_objects[0].values())[0]\n",
    "        annotation.pop(\"objects\")\n",
    "        annotations2.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(annotations2))\n",
    "print(annotations2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing representations\n",
    "\n",
    "Load a headless pre-trained ResNet50. There are a few ways you can do it:\n",
    "- using the previous ResNet_fc and removing the last two layers (Convolution and Softmax)\n",
    "- loading a headless ResNet from Keras and removing the AveragePooling layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "\n",
    "def predict_batch(model, img_batch_path, img_size=None):\n",
    "    img_list = []\n",
    "\n",
    "    for im_path in img_batch_path:\n",
    "        img = imread(im_path)\n",
    "        if img_size:\n",
    "            img = imresize(img,img_size)\n",
    "\n",
    "        img = img.astype('float32')\n",
    "        img_list.append(img)\n",
    "    try:\n",
    "        img_batch = np.stack(img_list, axis=0)\n",
    "    except:\n",
    "        raise ValueError('when img_size and crop_size are None, images'\n",
    "                ' in image_paths must have the same shapes.')\n",
    "\n",
    "    batch = preprocess_input(img_batch)\n",
    "    return model.predict(x = img_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "model = ResNet50(include_top=False)\n",
    "input = model.layers[0].input\n",
    "\n",
    "# Remove the average pooling layer!\n",
    "output = model.layers[-2].output\n",
    "headless_conv = Model(input = input, output = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test the model\n",
    "output = predict_batch(headless_conv, [\"dog.jpg\"], (1000, 224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute representations on all images in our annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_representations(annotations):\n",
    "    # Computing representations\n",
    "\n",
    "    batch_size = 32\n",
    "    batches = []\n",
    "\n",
    "    for a_idx in range(len(annotations)//32+1):\n",
    "        batch_bgn = a_idx*32\n",
    "        batch_end = min(len(annotations), (a_idx+1)*32)\n",
    "        img_names = []\n",
    "        for annotation in annotations[batch_bgn:batch_end]:\n",
    "            img_names.append(\"VOCdevkit/VOC2007/JPEGImages/\" + annotation[\"filename\"])\n",
    "        batch = predict_batch(headless_conv, img_names, (224, 224))\n",
    "        batches.append(batch)\n",
    "        print(\"batch \" +str(a_idx) + \" prepared\") \n",
    "    return np.vstack(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes representations (warning this may take some time!)\n",
    "# reprs = compute_representations(annotations)\n",
    "#import h5py\n",
    "\n",
    "# Serialize representations\n",
    "#h5f = h5py.File('representaions.h5', 'w')\n",
    "#h5f.create_dataset('reprs', data=reprs)\n",
    "#h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading serialized representations\n",
    "\n",
    "- The representations won't be fine-tuned, so we may save them so that we won't have to recompute them each time\n",
    "- to retrieve large data files, we h5 compressed file format, using h5py as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load pre-calculated representations\n",
    "h5f = h5py.File('representaions.h5','r')\n",
    "reprs = h5f['reprs'][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ground truth from annotation\n",
    "\n",
    "Our goal is to build the `y_true` tensor that will be compared to the output of the model\n",
    "- the image is resized to a fixed 224x224, so need to be the boxes coordinates\n",
    "- What are the output sizes of the model, for such a size of input image?\n",
    "\n",
    "#### classes\n",
    "- Convert an annotation format to tensor for classes:\n",
    " - each annotated object will be mapped to a single position in the `(7, 7)` grid\n",
    " - the class labels are mapped to `'background':0, 'cat': 1, 'dog': 2`\n",
    " \n",
    "#### boxes\n",
    "- Convert an annotation format to tensor for boxes:\n",
    " - each annotated object has a default box around the position of the object on the `(7, 7)` grid\n",
    " - the coordinates of the box represent the following:\n",
    "   - horizontal offset of center (between the default box and the ground truth box)\n",
    "   - vertical offset of center (between the default box and the ground truth box)\n",
    "   - difference of width (between the default box 32 and the ground truth box)\n",
    "   - difference of height (between the default box 32 and the ground truth box)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2idx = {'cat': 1, 'dog': 2}\n",
    "idx2label = {v:k for k,v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_resize = 224\n",
    "grid_size = 7\n",
    "box_size = img_resize // grid_size\n",
    "\n",
    "def box_center(x,y):\n",
    "    return (box_size/2 + x * box_size, box_size/2 + y * box_size)\n",
    "\n",
    "def convert_to_ground_truth(annotations):\n",
    "    all_boxes = []\n",
    "    all_cls = []\n",
    "    for idx, annotation in enumerate(annotations):\n",
    "        cls = np.zeros((grid_size,grid_size,3))\n",
    "        boxes = np.zeros((grid_size,grid_size,4))\n",
    "        size = annotation[\"size\"]\n",
    "        objects = annotation[\"objects\"]\n",
    "        for obj in objects: \n",
    "            for k, v in obj.items():\n",
    "                lbl = label2idx[k]\n",
    "                x1,y1,x2,y2 = (v[0]*img_resize/size[0], v[1]*img_resize/size[1], \n",
    "                               v[2]*img_resize/size[0], v[3]*img_resize/size[1])   \n",
    "                c = ((x2 + x1)/2, (y2 + y1)/2)\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                cx_id = int(c[0] / box_size)\n",
    "                cy_id = int(c[1] / box_size)\n",
    "                cls[cx_id,cy_id,lbl] = 1.0\n",
    "                b_center = box_center(cx_id, cy_id)\n",
    "                boxes[cx_id,cy_id,0] = (c[0] - b_center[0])/32.\n",
    "                boxes[cx_id,cy_id,1] = (c[1] - b_center[1])/32.\n",
    "                boxes[cx_id,cy_id,2] = 1 - w/32.\n",
    "                boxes[cx_id,cy_id,3] = 1 - h/32.\n",
    "        all_boxes.append(boxes)\n",
    "        all_cls.append(cls)\n",
    "    return np.stack(all_cls, axis=0), np.stack(all_boxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_resize = 224\n",
    "grid_size = 7\n",
    "box_size = img_resize // grid_size\n",
    "dim_output = 7 # number of bounding box * (4+1) + number of classes\n",
    "label2idx = {'cat': 0, 'dog': 1}\n",
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "\n",
    "def box_center(x,y):\n",
    "    return (box_size/2 + x * box_size, box_size/2 + y * box_size)\n",
    "\n",
    "def convert_to_yolo_ground_truth(annotations):\n",
    "    all_outputs = []\n",
    "    \n",
    "    for idx, annotation in enumerate(annotations):\n",
    "        output = np.zeros((grid_size,grid_size,dim_output))\n",
    "        size = annotation[\"size\"]\n",
    "        objects = annotation[\"objects\"]\n",
    "        for obj in objects: \n",
    "            for k, v in obj.items():\n",
    "                lbl = label2idx[k]\n",
    "                \n",
    "                # normalized coordinates of gt box\n",
    "                x1,y1,x2,y2 = (v[0]*img_resize/size[0], v[1]*img_resize/size[1], \n",
    "                               v[2]*img_resize/size[0], v[3]*img_resize/size[1])   \n",
    "                \n",
    "                # center\n",
    "                c = ((x2 + x1)/2, (y2 + y1)/2)\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                \n",
    "                # select most probable grid box\n",
    "                cx_id = int(c[0] / box_size)\n",
    "                cy_id = int(c[1] / box_size)\n",
    "                \n",
    "                # assign confidence for that box and label\n",
    "                output[cx_id, cy_id, 4] = 1.0\n",
    "                output[cx_id, cy_id, 5 + lbl] = 1.0\n",
    "                \n",
    "                # center and width\n",
    "                b_center = box_center(cx_id, cy_id)\n",
    "                output[cx_id,cy_id,0] = (c[0] - b_center[0])/32.\n",
    "                output[cx_id,cy_id,1] = (c[1] - b_center[1])/32.\n",
    "                output[cx_id,cy_id,2] = 1 - w/32.\n",
    "                output[cx_id,cy_id,3] = 1 - h/32.\n",
    "        all_outputs.append(output)\n",
    "    return np.stack(all_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_gt = convert_to_ground_truth(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes, boxes = convert_to_yolo_ground_truth(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"classes and boxes shapes:\", classes.shape, boxes.shape)\n",
    "print(\"classes and boxes shapes:\", yolo_gt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting output of model\n",
    "\n",
    "Interpreting the output of the model is going from the output tensors to a set of classes (with confidence) and boxes coordinates. It corresponds to reverting the previous process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpret_output(cls, boxes, threshold=0.7, img_size=(500,333)):\n",
    "    idx_positive = np.nonzero(cls > threshold)\n",
    "    output = []\n",
    "    for idx in range(len(idx_positive[0])):\n",
    "        x = idx_positive[0][idx]\n",
    "        y = idx_positive[1][idx]\n",
    "        classname = idx_positive[2][idx]\n",
    "        if classname==0:\n",
    "            continue\n",
    "        boxes_raw_cx = x * box_size + box_size/2\n",
    "        boxes_raw_cy = y * box_size + box_size/2\n",
    "        cx = boxes_raw_cx + boxes[x,y,0] * box_size\n",
    "        cy = boxes_raw_cy + boxes[x,y,1] * box_size\n",
    "        w = box_size * (1 - boxes[x,y,2])\n",
    "        h = box_size * (1 - boxes[x,y,3])\n",
    "        small_box = [max(0, cx - w/2), max(0, cy - h/2), \n",
    "                     min(img_resize, cx + w/2), min(img_resize, cy + h/2)]\n",
    "        fullsize_box = [int(small_box[0] * img_size[0] / img_resize), int(small_box[1] * img_size[1] / img_resize),\n",
    "                        int(small_box[2] * img_size[0] / img_resize), int(small_box[3] * img_size[1] / img_resize)]\n",
    "        #todo check bounds in image\n",
    "        output.append({idx2label[classname]: fullsize_box, \"confidence\":cls[x,y, classname]})\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpret_yolo_output(yolo_output, threshold=0.7, img_size=(500,333)):\n",
    "    proba_classes = np.multiply(yolo_output[:,:,5:], yolo_output[:,:,4:5])\n",
    "    idx_positive = np.nonzero(proba_classes > threshold)\n",
    "    output = []\n",
    "    for idx in range(len(idx_positive[0])):\n",
    "        x = idx_positive[0][idx]\n",
    "        y = idx_positive[1][idx]\n",
    "        classname = np.argmax(proba_classes[x,y])\n",
    "        confidence = np.max(proba_classes[x,y])\n",
    "        boxes_raw_cx = x * box_size + box_size/2\n",
    "        boxes_raw_cy = y * box_size + box_size/2\n",
    "        cx = boxes_raw_cx + yolo_output[x,y,0] * box_size\n",
    "        cy = boxes_raw_cy + yolo_output[x,y,1] * box_size\n",
    "        w = box_size * (1 - yolo_output[x,y,2])\n",
    "        h = box_size * (1 - yolo_output[x,y,3])\n",
    "        small_box = [max(0, cx - w/2), max(0, cy - h/2), \n",
    "                     min(img_resize, cx + w/2), min(img_resize, cy + h/2)]\n",
    "        fullsize_box = [int(small_box[0] * img_size[0] / img_resize), int(small_box[1] * img_size[1] / img_resize),\n",
    "                        int(small_box[2] * img_size[0] / img_resize), int(small_box[3] * img_size[1] / img_resize)]\n",
    "        #todo check bounds in image\n",
    "        output.append({idx2label[classname]: fullsize_box, \"confidence\":confidence})\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sanity check: convert annotation to ground truth, then interpret the tensors\n",
    "print(annotations[1])\n",
    "output = interpret_yolo_output(yolo_gt[1], threshold=0.7, img_size=annotations[0][\"size\"])\n",
    "print(output)\n",
    "print(\"iou: \"+str(iou(annotations[1][\"objects\"][0][\"dog\"], output[0][\"dog\"])))\n",
    "match_pred_and_gt(output, annotations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    " \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    " \n",
    "    # compute the area of each box\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of areas - the interesection area \n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "    \n",
    "    \n",
    "def match_iou(listA, listB):\n",
    "    y_indices = []\n",
    "    for ix,x in enumerate(listA):\n",
    "        values = [iou(x,y) for iy, y in enumerate(listB) if iy not in y_indices]\n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "        y_indices.append(values.index(max(values)))\n",
    "    pairs = list(zip(range(len(listA)), y_indices))\n",
    "    return pairs\n",
    "\n",
    "def match_pred_and_gt(prediction, ground_truth):\n",
    "    dic = {}\n",
    "    for obj in ground_truth[\"objects\"]:        \n",
    "        for k, v in obj.items():\n",
    "            if k not in dic:\n",
    "                dic[k] = ([v], [])\n",
    "            else:\n",
    "                dic[k][0].append(v)\n",
    "    for obj in prediction:\n",
    "        for k, v in obj.items():\n",
    "            if k!=\"confidence\":\n",
    "                if k not in dic:\n",
    "                    dic[k] = ([], [v])\n",
    "                else:\n",
    "                    dic[k][1].append(v)\n",
    "    \n",
    "    final_dic = {}\n",
    "    for k,v in dic.items():\n",
    "        pairs = match_iou(v[0], v[1])\n",
    "        final_dic[k] = (pairs, [iou(v[0][x],v[1][y]) for x,y in pairs])\n",
    "    return final_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sanity check: convert annotation to ground truth, then interpret the tensors\n",
    "print(annotations[1])\n",
    "output = interpret_output(classes[0], boxes[0], threshold=0.7, img_size=annotations[0][\"size\"])\n",
    "print(output)\n",
    "print(\"iou: \"+str(iou(annotations[0][\"objects\"][0][\"dog\"], output[0][\"dog\"])))\n",
    "match_pred_and_gt(output, annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-shot model\n",
    "\n",
    "A very straightforward single-shot detection model, much alike YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def masked_mse(y_true, y_pred):\n",
    "    masks = K.not_equal(y_true, 0.)\n",
    "    return K.mean(K.square(y_pred - y_true) * K.cast(masks, \"float32\"), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    # object\n",
    "    obj = K.cast(K.not_equal(y_true[:,:,:,4:5], 0.), \"float32\")\n",
    "    \n",
    "    # no_object\n",
    "    noobj = K.cast(K.not_equal(y_true[:,:,:,4:5], 1.), \"float32\")\n",
    "    \n",
    "    bbox_loss = K.mean(K.square(y_pred[:,:,:,0:3] - y_true[:,:,:,0:3]) * obj, axis=-1)\n",
    "    conf_loss = K.mean(K.square(y_pred[:,:,:,4:5] - y_true[:,:,:,4:5]) * obj, axis=-1)\n",
    "    conf_loss_noobj =  K.mean(K.square(y_pred[:,:,:,4:5] - y_true[:,:,:,4:5]) * noobj, axis=-1)\n",
    "    class_loss = K.mean(K.square(y_pred[:,:,:,5:] - y_true[:,:,:,5:]) * obj, axis=-1)\n",
    "    \n",
    "    return bbox_loss * 5 + conf_loss + conf_loss_noobj * 0.5 + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou_K(coords, coords_pred):\n",
    "    # predictions\n",
    "    centers = coords[:,:,:,0:2]\n",
    "    wh = coords[:,:,:,2:4]\n",
    "    upleft = centers - (wh * .5) # [batch, S, S, 2]\n",
    "    botright  = centers + (wh * .5) # [batch, S, S, 2]\n",
    "    area_pred = wh[:,:,:,0:1] * wh[:,:,:,1:2]\n",
    "    \n",
    "    # true\n",
    "    true_centers = coords[:,:,:,0:2]\n",
    "    true_wh = coords[:,:,:,2:4]\n",
    "    true_upleft = centers - (wh * .5) # [batch, S, S, 2]\n",
    "    true_botright  = centers + (wh * .5) # [batch, S, S, 2]\n",
    "    true_area = wh[:,:,:,0:1] * wh[:,:,:,1:2]\n",
    "    \n",
    "    # calculate the intersection areas\n",
    "    intersect_upleft   = K.maximum(upleft, true_upleft) \n",
    "    intersect_botright = K.minimum(botright , true_botright)\n",
    "    intersect_wh = intersect_botright - intersect_upleft\n",
    "    intersect_wh = K.maximum(intersect_wh, 0.0)\n",
    "    intersect = intersect_wh[:,:,:,0:1] * intersect_wh[:,:,:,1:2]\n",
    "    \n",
    "    # calculate the best IOU, set 0.0 confidence for worse boxes\n",
    "    iou = intersect / (true_area + area_pred - intersect)\n",
    "    best_box = K.equal(iou, K.max(iou, axis=2, keepdims=True))\n",
    "    best_box = K.cast(best_box, \"float32\")\n",
    "    return best_box\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    \n",
    "    # Extract the coordinate prediction from net.out\n",
    "    coords = y_pred[:, :, :, 0:4]\n",
    "    conf = y_pred[:, :, :, 4:5]\n",
    "    classes = y_pred[:, :, :, 5:]\n",
    "\n",
    "    true_coords = y_pred[:, :, :, 0:4]\n",
    "    true_conf = y_pred[:, :, :, 4:5]\n",
    "    true_classes = y_pred[:, :, :, 5:]\n",
    "    \n",
    "    best_box = iou_K(coords, true_coords)\n",
    "    noobj_box = (1. - best_box) # no object\n",
    "    \n",
    "    bbox_loss = K.mean(K.square(y_pred[:,:,:,0:4] - y_true[:,:,:,0:4]) * best_box, axis=-1)\n",
    "    conf_loss = K.mean(K.square(y_pred[:,:,:,4:5] - y_true[:,:,:,4:5]) * best_box, axis=-1)\n",
    "    conf_loss_noobj =  K.mean(K.square(y_pred[:,:,:,4:5] - y_true[:,:,:,4:5]) * noobj_box, axis=-1)\n",
    "    class_loss = K.mean(K.square(y_pred[:,:,:,5:] - y_true[:,:,:,5:]) * best_box, axis=-1)\n",
    "    \n",
    "    return bbox_loss * 5 + conf_loss + conf_loss_noobj * 0.1 + class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.objectives import mean_squared_error, binary_crossentropy\n",
    "from keras.layers import Input, Convolution2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def ssd_model(num_classes, num_boxes):\n",
    "    model_input = Input(shape=(None,None,2048))\n",
    "    x = Dropout(0.3)(model_input)\n",
    "    head_classes = Convolution2D(num_classes, 1, 1, activation='sigmoid', name='classes')(x)\n",
    "    #head_classes = Convolution2D(num_classes, 1, 1, name='classes')(model_input)\n",
    "    #head_classes = SoftmaxMap(axis=-1)(head_classes)\n",
    "    \n",
    "    head_boxes = Convolution2D(4*num_boxes, 1, 1, name='boxes')(x)\n",
    "    \n",
    "    model = Model(model_input, output = [head_classes, head_boxes], name=\"resnet_ssd\")\n",
    "    model.compile(optimizer=\"adam\", loss=[binary_crossentropy, masked_mse], \n",
    "                  loss_weights=[1., 0.4]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yolo(num_boxes, num_classes):\n",
    "    model_input = Input(shape=(None,None,2048))\n",
    "    x = Convolution2D(128, 1, 1, activation='relu', name='conv_hidden')(model_input)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    output = Convolution2D(5*num_boxes + num_classes, 1, 1, name='boxes')(x)\n",
    "    \n",
    "    model = Model(model_input, output = output, name=\"resnet_ssd\")\n",
    "    rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=rms, loss=yolo_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = yolo(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ssd_model(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 64\n",
    "batch_size = 10\n",
    "inputs = reprs[0:num]\n",
    "outputs = yolo_gt[0:num]\n",
    "print(inputs.shape, outputs.shape)\n",
    "out = model.predict(x=inputs)\n",
    "print(out.shape)\n",
    "model.fit(inputs, outputs, batch_size=batch_size, nb_epoch=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(out[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sanity check: overfit on a batch of annotation\n",
    "num = 64\n",
    "batch_size = 10\n",
    "inputs = reprs[0:num]\n",
    "outputs = [classes[0:num], boxes[0:num]]\n",
    "print(inputs.shape, outputs[0].shape, outputs[1].shape)\n",
    "out = model.predict(x=inputs)\n",
    "print(out[0].shape, out[1].shape)\n",
    "model.fit(inputs, outputs, batch_size=batch_size, nb_epoch=50)\n",
    "\n",
    "#model.fit(inputs, outputs, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_annotations(img_path, annotation):\n",
    "    img = imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    currentAxis = plt.gca()\n",
    "    for dic in annotation:\n",
    "        color = \"red\" # ground truth\n",
    "        conf = \"gt \"\n",
    "        box = []\n",
    "        text = \"error\"\n",
    "        if 'confidence' in dic:\n",
    "            conf = '{:0.2f} '.format(dic['confidence'])\n",
    "            color = \"green\"\n",
    "        for k,v in dic.items():\n",
    "            if k!='confidence':\n",
    "                text = k\n",
    "                bbox = v\n",
    "        if text==\"error\":\n",
    "            print(\"error!\")\n",
    "            continue\n",
    "        display_txt = conf + text\n",
    "        coords = (bbox[0], bbox[1]), bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(bbox[0], bbox[1], display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_ground_truth(index):\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     annotations[index][\"objects\"])\n",
    "\n",
    "def display_yolo_prediction(index, threshold=0.5):\n",
    "    res = model.predict(reprs[index][np.newaxis,])\n",
    "    output = interpret_yolo_output(res[0], threshold=threshold, img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     output)\n",
    "    \n",
    "def display_prediction(index, threshold=0.5):\n",
    "    res = model.predict(reprs[index][np.newaxis,])\n",
    "    output = interpret_output(res[0][0], res[1][0], threshold=threshold, img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     output)\n",
    "\n",
    "def display_both(index, threshold=0.5):\n",
    "    res = model.predict(reprs[index][np.newaxis,])\n",
    "    output = interpret_yolo_output(res[0], threshold=threshold, img_size=annotations[index][\"size\"])\n",
    "    plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     output + annotations[index][\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = model.predict(reprs[1][np.newaxis,])\n",
    "print(np.max(out[0][:,:,6]*out[0][:,:,4]), np.max(out[0][:,:,4]), np.max(out[0][:,:,5]), np.max(out[0][:,:,6]))\n",
    "plt.imshow(out[0][:,:,5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_both(-7,threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep last examples for test\n",
    "test_num = reprs.shape[0] // 10\n",
    "train_num = reprs.shape[0] - test_num\n",
    "print(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inputs_train = reprs[0:train_num]\n",
    "outputs_train = yolo_gt[0:train_num]\n",
    "model.fit(inputs_train, outputs_train, batch_size=batch_size, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_acc(train=True)\n",
    "compute_acc(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_both(36, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_cls, out_box = model.predict(reprs[28][np.newaxis,])\n",
    "out_cls.shape, out_box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, truth, threshold = 0.1):\n",
    "    count_valid = 0\n",
    "    count_total = 0\n",
    "    for p, t in zip(pred, truth):\n",
    "        for k in label2idx.keys():\n",
    "            pred_coords = [dic[k] for dic in p if k in dic]\n",
    "            true_coords = [dic[k] for dic in t if k in dic]\n",
    "            if(len(pred_coords)==0 or len(true_coords)==0):\n",
    "                continue\n",
    "            matches = match_iou(pred_coords, true_coords)\n",
    "            for match in matches:\n",
    "                iou_value = iou(pred_coords[match[0]], true_coords[match[1]])\n",
    "                if iou_value > threshold:\n",
    "                    count_valid = count_valid + 1\n",
    "        count_total = count_total + max(len(t), len(p))\n",
    "    return count_valid / count_total\n",
    "    \n",
    "        \n",
    "def mAP():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_acc(train=True):\n",
    "    if train:\n",
    "        beg, end = 0, (9 * len(annotations))\n",
    "        txt = \"train\"\n",
    "    else:\n",
    "        beg, end = (9 * len(annotations)) // 10, len(annotations) \n",
    "        txt = \"test\"\n",
    "    res = model.predict(reprs[beg:end])\n",
    "    outputs = []\n",
    "    for index, r in enumerate(res):\n",
    "        output = interpret_yolo_output(r, threshold=0.2, img_size=annotations[][\"size\"])\n",
    "        outputs.append(output)\n",
    "    acc = accuracy(output, [ann[\"objects\"] for ann in annotations[beg:end]], threshold=0.5)\n",
    "    \n",
    "    print(txt + ' accuracy: {:0.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_acc(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 36\n",
    "plot_annotations(\"VOCdevkit/VOC2007/JPEGImages/\" + annotations[index][\"filename\"], \n",
    "                     annotations[index][\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
