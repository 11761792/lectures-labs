{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to better understand how stochastic gradient works. We fit a very simple non-convex model to data generated from a linear ground truth model.\n",
    "\n",
    "We will also observe how the (stochastic) loss landscape changes when selecting different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated from a simple model:\n",
    "$$y=  2x + \\epsilon$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, .1)$\n",
    "- $x \\sim \\mathcal{U}(-1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_ground_truth(n_samples=100):\n",
    "    x = torch.FloatTensor(n_samples, 1).uniform_(-1, 1)\n",
    "    epsilon = torch.FloatTensor(n_samples, 1).normal_(0, .1)\n",
    "    y = 2 * x + epsilon\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = sample_from_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a minimal single hidden layer perceptron model with a single hidden unit and no bias. The model has two tunable parameters $w_1$, and $w_2$, such that:\n",
    "\n",
    "$$f(x) = w_1 \\cdot \\sigma(w_2 \\cdot x)$$\n",
    "\n",
    "where $\\sigma$ is the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, w=None):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.w1 = Parameter(torch.FloatTensor((1, )))\n",
    "        self.w2 = Parameter(torch.FloatTensor((1, )))\n",
    "        if w is None:\n",
    "            self.reset_parameters()\n",
    "        else:\n",
    "            self.set_parameters(w)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.w1.data.uniform_(-.1, .1)\n",
    "        self.w2.data.uniform_(-.1, .1)\n",
    "\n",
    "    def set_parameters(self, w):\n",
    "        self.w1.data[0] = w[0]\n",
    "        self.w2.data[0] = w[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w1 * relu(self.w2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we define a function to sample from and plot loss landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grids(x, y, grid_size=100):\n",
    "    n_samples = len(x)\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    # Grid logic\n",
    "    x_max, y_max, x_min, y_min = 3, 3, -3, -3\n",
    "    w1 = np.linspace(x_min, x_max, grid_size, dtype=np.float32)\n",
    "    w2 = np.linspace(y_min, y_max, grid_size, dtype=np.float32)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    W = np.concatenate((W1[:, :, None], W2[:, :, None]), axis=2)\n",
    "    W = torch.from_numpy(W)\n",
    "\n",
    "    # We will store the results in this tensor\n",
    "    grids = torch.FloatTensor(n_samples, grid_size, grid_size)\n",
    "\n",
    "    # Make x a variable. volatile=True tells pytorch to keep no\n",
    "    # information for gradient computation.\n",
    "    x = Variable(x, volatile=True)\n",
    "    y = Variable(y, volatile=True)\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            model = SimpleMLP(W[i, j])\n",
    "            pred = model(x)\n",
    "            loss = mse_loss(pred, y, reduce=False).data\n",
    "            grids[:, i, j] = loss\n",
    "    return W1, W2, grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `G[k, i, j]` holds the single sample loss value $\\ell(f(w_1 = i , w_2 = j, x_k), y_k)$\n",
    "\n",
    "- `G_mean[i, j]` corresponds to the empirical risk:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{k=1}^{n} \\ell(f(w_1 = i , w_2 = j, x_k), y_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, G = make_grids(x, y)\n",
    "G_mean = torch.mean(G, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our train loop and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "def train(x, y, init, lr=.1):\n",
    "    model = SimpleMLP(init)\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    iterate_rec = []\n",
    "    grad_rec = []\n",
    "    for this_x, this_y in zip(x, y):\n",
    "        this_x = this_x[None, :]\n",
    "        this_y = this_y[None, :]\n",
    "        this_x = Variable(this_x)\n",
    "        this_y = Variable(this_y)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(this_x)\n",
    "        loss = mse_loss(pred, this_y)\n",
    "        loss.backward()\n",
    "        iterate_rec.append([model.w1.data[0], model.w2.data[0]])\n",
    "        grad_rec.append([model.w1.grad.data[0], model.w2.grad.data[0]])\n",
    "        optimizer.step()\n",
    "    return np.array(iterate_rec), np.array(grad_rec)\n",
    "\n",
    "iterate_rec, grad_rec = train(x, y, lr=.05, init=torch.FloatTensor([1.2, -2.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the empirical mean on the left side, and the sample loss at iteration $k$ on the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_map(sample):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "    ax1.contourf(W1, W2, torch.log(G[sample]))\n",
    "    ax1.scatter(iterate_rec[sample, 0], iterate_rec[sample, 1], color='orange')\n",
    "    ax1.arrow(iterate_rec[sample, 0], iterate_rec[sample, 1], -5 * grad_rec[sample, 0], -5 * grad_rec[sample, 1],\n",
    "      head_width=0.1, head_length=0.2, fc='orange', ec='orange')\n",
    "    ax2.contourf(W1, W2, torch.log(G_mean))\n",
    "    ax2.plot(iterate_rec[:sample, 0], iterate_rec[:sample, 1], linestyle='-', marker='o', markersize=8,\n",
    "             color='orange', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "interactive_plot = interactive(plot_map, sample=(0, 99))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe and comment. Perform interesting change in the models and the initialisation to observe interesting behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
