{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to better understand how stochastic gradient works. We fit a very simple non-convex model to data generated from a linear model, and observe how the loss landscape changes when selecting different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated from a simple model: $y=  2x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, .1)$, $x \\in \\mathcal{U}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100):\n",
    "    x = torch.FloatTensor(n_samples, 1).uniform_(-1, 1)\n",
    "    epsilon = torch.FloatTensor(n_samples, 1).normal_(0, .1)\n",
    "    y = 2 * x + epsilon\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a model with two parameters $w_1$, and $w_2$, such that $f(x) = w_1 \\sigma(w_2 x)$, where $\\sigma$ is the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, w: torch.FloatTensor=None) -> None:\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.w1 = Parameter(torch.FloatTensor((1, )))\n",
    "        self.w2 = Parameter(torch.FloatTensor((1, )))\n",
    "        if w is None:\n",
    "            self.reset_parameters()\n",
    "        else:\n",
    "            self.set_parameters(w)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        self.w1.data.uniform_(-.1, .1)\n",
    "        self.w2.data.uniform_(-.1, .1)\n",
    "\n",
    "    def set_parameters(self, w: torch.FloatTensor) -> None:\n",
    "        self.w1.data[0] = w[0]\n",
    "        self.w2.data[0] = w[1]\n",
    "\n",
    "    def forward(self, x: Variable) -> Variable:\n",
    "        return self.w1 * relu(self.w2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we define a function to sample from and plot loss landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grids(x: torch.FloatTensor, y: torch.FloatTensor, grid_size=100):\n",
    "    n_samples = len(x)\n",
    "    assert(len(x) == len(y))\n",
    "\n",
    "    # Grid logic\n",
    "    x_max, y_max, x_min, y_min = 3, 3, -3, -3\n",
    "    w1 = np.linspace(x_min, x_max, grid_size, dtype=np.float32)\n",
    "    w2 = np.linspace(y_min, y_max, grid_size, dtype=np.float32)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    W = np.concatenate((W1[:, :, None], W2[:, :, None]), axis=2)\n",
    "    W = torch.from_numpy(W)\n",
    "\n",
    "    # We will store the results in this tensor\n",
    "    grids = torch.FloatTensor(n_samples, grid_size, grid_size)\n",
    "\n",
    "    # Make x a variable\n",
    "    x = Variable(x, volatile=True)  # volatile=True tells pytorch to keep no information for gradient computation\n",
    "    y = Variable(y, volatile=True)\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            model = SimpleMLP(W[i, j])\n",
    "            pred = model(x)\n",
    "            loss = mse_loss(pred, y, reduce=False).data\n",
    "            grids[:, i, j] = loss\n",
    "    return W1, W2, grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`G[k, i, j]` holds the value $\\ell(f(w_1 = i , w_2 = j, x_k), y_k)$, while\n",
    "`G_mean[i, j]` corresponds to the empirical risk\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{k=1}^{n} \\ell(f(w_1 = i , w_2 = j, x_k), y_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, G = make_grids(x, y)\n",
    "G_mean = torch.mean(G, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our train loop and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "def train(x, y, init, lr=.1):\n",
    "    model = SimpleMLP(init)\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    iterate_rec = []\n",
    "    grad_rec = []\n",
    "    for this_x, this_y in zip(x, y):\n",
    "        this_x = this_x[None, :]\n",
    "        this_y = this_y[None, :]\n",
    "        this_x = Variable(this_x)  # volatile=True tells pytorch to keep no information for gradient computation\n",
    "        this_y = Variable(this_y)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(this_x)\n",
    "        loss = mse_loss(pred, this_y)\n",
    "        loss.backward()\n",
    "        iterate_rec.append([model.w1.data[0], model.w2.data[0]])\n",
    "        grad_rec.append([model.w1.grad.data[0], model.w2.grad.data[0]])\n",
    "        optimizer.step()\n",
    "    return np.array(iterate_rec), np.array(grad_rec)\n",
    "\n",
    "iterate_rec, grad_rec = train(x, y, lr=.05, init=torch.FloatTensor([1.2, -2.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the empirical mean on the left side, and the sample loss at iteration $k$ on the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_map(sample):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "    ax1.contourf(W1, W2, torch.log(G[sample]))\n",
    "    ax1.scatter(iterate_rec[sample, 0], iterate_rec[sample, 1], color='orange')\n",
    "    ax1.arrow(iterate_rec[sample, 0], iterate_rec[sample, 1], -5 * grad_rec[sample, 0], -5 * grad_rec[sample, 1],\n",
    "      head_width=0.1, head_length=0.2, fc='orange', ec='orange')\n",
    "    ax2.contourf(W1, W2, torch.log(G_mean))\n",
    "    ax2.plot(iterate_rec[:sample, 0], iterate_rec[:sample, 1], linestyle='-', marker='o', markersize=8,\n",
    "             color='orange', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d12ba30cc96490b99d661951bc3a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=49, description='sample', max=99), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "interactive_plot = interactive(plot_map, sample=(0, 99))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe and comment. Perform interesting change in the models and the initialisation to observe interesting behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
