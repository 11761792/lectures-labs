<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .right-column {
       width: 50%;
       float: right;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
     }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
      <textarea id="source">
class: center, middle

# Unsupervised learning and Generative models

Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/heuritech-logo.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]


---
## Outline

### Unsupervised learning

--

### Autoencoders

--

### Generative Adversarial Networks

---
class: middle, center

# Unsupervised learning

---
## Unsupervised learning

Goal of unsupervised learning is to **find underlying structrure** in data, with several aims:

--

- clustering

- building representations of data for a downstream supervised task (e.g. classification)

--

For complex data (text, image, sound, ...), there is plenty of hidden latent structure we hope to capture

--

- **Image data**: find low dimension, semantic visual representations, independant of variation sources

- **Text data**: find fixed size, dense semantic representation of data

---
## Graal of unsupervised learning

Find a low dimension space which captures all the **variation** of data and **disantangles** the different latent factors underlying the data

.center[
          <img src="images/latent_infogan.png" style="width: 650px;" />
]

.footnote.small[
Chen, Xi, et al. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. NIPS, 2016.
]
---
## Self-supervised learning

find smart ways to build supervision, exploiting problem knowledge and regularities

--

Use text structure to create supervision

- Word2Vec, Skip-thought vectors, language models

--

Can we do the same for image, video, etc?

---
## Self-supervised learning

.center[
          <img src="images/gupta1.png" style="width: 380px;" />
]


.footnote.small[
Doersch, Carl, Abhinav Gupta, and Alexei A. Efros. "Unsupervised visual representation learning by context prediction." ICCV 2015.
]

--

- Predict patches arrangement in images: 8 class classifier
- Siamese architecture for the two patches + concat

---
# Self-supervised from video

.center[
          <img src="images/gupta2.png" style="width: 380px;" />
]

.footnote.small[
Wang, Xiaolong, and Abhinav Gupta. "Unsupervised learning of visual representations using videos." ICCV 2015.
]
--

- Collect pairs of similar objects from videos

--
- Train a siamese net with positive pairs = similar objects detected

--
- Hard pairs mining: find objects with large movement

---
class: middle, center

# Autoencoders

---
## Autoencoder

.center[
          <img src="images/autoencoder.png" style="width: 420px;" />
]

--

Self supervision: reconstruction loss of the input, usually:

$$l(x, y = x) = || f(x) - x ||^2_2$$

--

Binary crossentropy is also used

---
## Autoencoder

.center[
          <img src="images/autoencoder.png" style="width: 420px;" />
]

Keeping the latent code $z$ small forces the network to learn a "smart" compression of the data, not just an identity function

--

Encoder and decoder can have arbritrary architecture (CNNs, RNNs...)
---
## Sparse/Denoising Autoencoder

Adding a sparsity constraint on activations:

$$ ||encoder(x)||_1 \sim \rho, \rho = 0.05 $$

Learns sparse features, more easily interpretable

---
###

Results

---
## Reality Check

For image features, **ImageNet pretraining** is still **much better** than unsupervised

.center[
          <img src="images/results_features.png" style="width: 420px;" />
]

.footnote.small[
Pathak, Deepak, et al. Context encoders: Feature learning by inpainting. CVPR 2016.
]

--

Fine-tuned on Pascal VOC dataset

---

## VAE: Todo

---
class: middle, center

# Generative Adversarial Networks

---
## Generative Adversarial Networks

.center[
          <img src="images/gan_vanilla.jpg" style="width: 260px;" />
]

.footnote.small[
Goodfellow, Ian, et al. Generative adversarial nets. NIPS 2014.
]

---
## GANs

Alternate training of a **generative network** $G$ and a **discrimininative network** $D$

--

- D tries to find out which example are generated or real
- G tries to fool D into thinking its generated examples are real

---
## GANs

Todo: plenty of explanation, 1D explanation

---
## Info GAN

.center[
          <img src="images/gan_info.jpg" style="width: 260px;" />
]

.footnote.small[
Chen, Xi, et al. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. NIPS, 2016.
]

--

.center[
          <img src="images/latent_infogan.png" style="width: 350px;" />
]
---
## DC-GAN

.center[
          <img src="images/dcgan.png" style="width: 560px;" />
]

.footnote.small[
Radford, Alec, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. 2015.
]
---
## Image generation research

.center[
          <img src="images/nvidia.gif" style="width: 360px;" />
]

All images are generated by walking through the latent space

.footnote.small[
Karras, Tero, et al. Progressive growing of gans for improved quality, stability, and variation. 2017.
]

---
## Super Resolution

.center[
          <img src="images/srgan.png" style="width: 630px;" />
]

"Perceptual" loss = combining pixel-wise loss mse-like loss with GAN loss

.footnote.small[
Ledig, Christian, et al. Photo-realistic single image super-resolution using a generative adversarial network. CVPR 2016.
]


---
## Domain Adversarial Training

.center[
          <img src="images/domain_adversarial.png" style="width: 630px;" />
]

.footnote.small[
Ganin, Yaroslav, et al. Domain-adversarial training of neural networks. JMLR 2016.
]
---
class: middle, center

# Lab 10: Room Painlev√© et Sauvy in 15min!

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
