<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .reset-column {
       overflow: auto;
        width: 100%;
     }
     .small { font-size: 0.2em; }
     .right-column {
       width: 50%;
       float: right;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
      }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Natural Language Processing with Deep Learning

Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/logo heuritech v2.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]


---
### Natural Language Processing

intro

---
# Outline

<br/>


### Word representation: Word2Vec

--

### Recurrent Neural Networks for Language Modelling 

--

### Attention Mechanism & Memory

---
class: middle, center

# Localisation

---
# Word representation

Vocabulary, ...
- Remember: embeddings

--

Application for text classification
- (example fastText)

---
# Transfer Learning with text

- similar to image, can we have word representations for transfer learning?
- unsupervised learning of word representation

---
# Distributional Hypothesis

Distributional Hypothesis (Harris, 1954):
*“words are characterised by the company that they keep”*

Main idea: learning word embeddings by predicting word contexts

--

Given a word e.g. “carrot” and any other word $w \in V$ predict
probability $P(w|\text{carrot})$ that $w$ occurs in the context of
“carrot”.

--

Unsupervised / self-supervised: no need of labels.

Supervision comes from context.

Requires a lot of text data to cover rare words correctly.

.footnote.small[
Word2vec mikolov
]
---
# Word2Vec: CBoW

.center[
<img src="images/word2vec_simple3.svg" style="width: 720px;" />
]

.center[
<img src="images/textcontext.svg" style="width: 620px;" />
]
---
# Word2Vec: CBoW

.center[
<img src="images/word2vec_simple4.svg" style="width: 720px;" />
]

.center[
<img src="images/textcontext.svg" style="width: 620px;" />
]
---
# Word2Vec: CBoW

.center[
<img src="images/word2vec_simple5.svg" style="width: 720px;" />
]

.center[
<img src="images/textcontext.svg" style="width: 620px;" />
]
---
# Word2Vec: CBoW

.center[
<img src="images/word2vec_simple2.svg" style="width: 720px;" />
]

.center[
<img src="images/textcontext.svg" style="width: 620px;" />
]
---
# Word2Vec: Skip Gram

--
add details hierarchical softmax VS negative sampling

---
# Related methods

- GloVe (Socher et al.) insert link
- PMI matrix factorization  (Levi & Golberg) relation with count based methods

---
# Evaluating representations

Always difficult to evaluate unsupervised tasks

- WordSim (Finkelstein et al.)
- SimLex-999 (Hill et al.)
- Word Analogies (Mikolov et al.)

---
# Word and Embeddings

### for text applications, inputs of Neural Networks are Embeddings

#### If you have little training data, or wide vocabulary not covered by data, use pre-trained embeddings (transfer learning)

--

#### Otherwise train your embeddings in the task (results in task-specific embeddings)

---
class:middle, center

# Recurrent Neural Networks for Language Modelling

---
# Language Models

assign a probability to a sequence of words

- $p(\text{I like cats}) > p(\text{I table cats})$
- $p(\text{I like cats}) > p(\text{like I cats})$

more informative than Bag of Words

--

Sequence modelling

$$
p(w\_n | w\_{n-1}, w\_{n-2}, ..., w\_0) 
$$

---
# Conditional Language Models

Many NLP problems can be expressed as Conditional Language Models:

**Translation:** $p(source | destination)$

*source/destination*:  "J'aime les chats" / "I like cats"

--

**Question Answering / dialogue:** $p( Answer | Question , Context )$

- *Context*: "John put 2 glasses on the table. Bob adds two more glasses"
- *Question*: "How many glasses are there?"
- *Answer*: "There are four glasses."

--

**Image Captionning:** $p( Caption | Image )$


---
# Fixed sequence size

.center[
<img src="images/fixedsize_mlp.svg" style="width: 400px;" />
]

---
# Recurrent Neural Network

.center[
<img src="images/unrolled_rnn.svg" style="width: 400px;" />
]

---
## Backpropagation through time

.center[
<img src="images/unrolled_rnn_backward.svg" style="width: 400px;" />
]

---
# Recurrent Neural Network

.center[
<img src="images/rnn_simple.svg" style="width: 200px;" />
]

--

Params:

- $W_{in} |V| \times h$ : embedding 
- $W_h h \times h$ : reccurent layer
- $W_{out} h \times K$ : output, can be $|V|$

---
# Recurrent Neural Network

$\{w_t\}$ sequence of words ( 1-hot encoded )

include picture

$$
x\_t = \text{Emb}(w\_t) = \mathbf{E} w\_t \\\\
h\_t = g(\mathbf{W\_h} h\_{t-1} + x\_t + b\_h)\\\\
y = \text{softmax}( \mathbf{W\_o} h\_t + b\_o )
$$

---
# GRU

Gated Recurrent Unit

.footnote.small[
Chung, Junyoung, et al. "Gated Feedback Recurrent Neural Networks." ICML 2015
]
---
# LSTM

.footnote.small[
Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 1997
]

- $\mathbf{ u} = \sigma(\mathbf{W^u} \cdot h\_{t-1} + \mathbf{I^u} \cdot x\_t + b^u)$

--
- $\mathbf{ f} = \sigma(\mathbf{W^f} \cdot h\_{t-1} + \mathbf{I^f} \cdot x\_t + b^f)$

--
- $\mathbf{ \tilde{C}} = \tanh(\mathbf{W^c} \cdot h\_{t-1} + \mathbf{I^c} \cdot x\_t + b^c)$

--
- $\mathbf{ C\_t} = \mathbf{f} \odot \mathbf{C\_{t-1}} + \mathbf{u} \odot \mathbf{ \tilde{C}}$

--
- $\mathbf{ o} = \sigma(\mathbf{W^o} \cdot h\_{t-1} + \mathbf{I^o} \cdot x\_t + b^o)$

--
- $\mathbf{ h\_t} = \mathbf{o} \odot \tanh(\mathbf{C\_t})$

--
- $y = \text{softmax}( \mathbf{W} \cdot h\_t + b )$

--

$W^u, W^f, W^c, W^o$ recurrent weights `H x H` <br/>
$I^u, I^f, I^c, I^o$ input weights `N x H`

---
input: sequence of words
output: single vector of dimension K, or sequence of vectors of dimension K

schema karpathy many-to-one, one-to-many, many-to-many
---
class: middle, center

# Lab 5: Room F503 and F900 in 15min!

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
